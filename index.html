<!DOCTYPE html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<head>
    <meta charset="utf-8">
    <!-- existing meta tags -->
    <!-- Google Analytics script goes here -->
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WLNGSQPW7E"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-WLNGSQPW7E');
    </script>
</head>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="PIN Project Page">
  <meta property="og:title" content="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs"/>
  <meta property="og:description" content="Project page of 'PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs'"/>
  <meta property="og:url" content="https://quva-lab.github.io/PIN/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/first.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="600"/>


  <meta name="twitter:title" content="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/first.png">
  <meta name="twitter:card" content="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="VLMs, object localisation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <!-- <div class="hero-body" style="background-color: #bdbaba;"> -->
        <!-- <div class="hero-body" style="background-image: url('static/images/background.avif'); background-size: cover; background-repeat: no-repeat; background-position: center center;"> -->
            <div class="hero-body" style="background-image: url(static/images/background.avif);">

      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
                <img src="static/images/pin2.png" alt="PIN Image" style="width:40px;height:40px;"> PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mdorkenwald.com/" target="_blank">Michael Dorkenwald</a>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/nimrod-barazani/" target="_blank">Nimrod Barazani</a>,
              </span>
              <span class="author-block">
                <a href="https://www.ceessnoek.info/" target="_blank">Cees Snoek*</a>,
              </span>
              <span class="author-block">
                <a href="https://yukimasano.github.io/" target="_blank">Yuki M. Asano*</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block" style="color: red;">Accepted to CVPR'24</span><br>
              
            <span class="author-block">University of Amsterdam<br>
            <span style="font-size: small;">*equal last author</span>
            </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.08657" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <!-- <span class="link-block">
                  <a href="https://colab.research.google.com/github/phlippe/BISCUIT/blob/main/demo.ipynb" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg style="color: white" role="img" height="100px" width="100px" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0646 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9297zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9316l2.3911 2.3911a3.6434 3.6434 0 0 1 5.0227.1271l1.7341-2.9737-.0997-.0802A7.033 7.033 0 0 0 7.07 4.9855zm15.0093 2.1721l-2.3892 2.3911a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.4067 2.4068a7.0362 7.0362 0 0 0 9.9456-9.9476zM1.932 7.1674a7.033 7.033 0 0 0-.002 9.6816l2.397-2.397a3.6434 3.6434 0 0 1-.004-4.8916zm7.664 7.4235c-1.38 1.3816-3.5863 1.411-5.0168.1134l-2.397 2.395c2.4693 2.3328 6.263 2.5753 9.0072.5455l.1368-.1115z" fill="white"></path></svg>
                  </span>
                  <span>Demo</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/QUVA-Lab/PIN/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                </span>

                <!-- Dataset link
                <span class="link-block">
                  <a href="https://zenodo.org/record/8027138" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-zenodo"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser" style="background-color: #fff6e6;">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%" style="background-color: white; padding: 30px;">
          <source src="static/videos/banner_video.mov"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <b>BISCUIT</b> learns causal variables from interactivate environments with low-level action information.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
        
      <div class="columns is-centered has-text-centered">
        
        <div class="column is-four-fifths">

          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. <strong>In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not utilizing any supervised detection data. </strong>
                To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons. 
            </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/first_page.jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 500px; padding-top: 10px;"/>
                <p style="width: 100%; max-width: 600px; margin: auto;">
                <i> We learn a single Positional Insert (PIN) for unlocking zero-shot object localisation abilities in a frozen Vision Language Model (VLM) without adding any additional heads or requiring supervised datasets.</i>
                </p>
                &nbsp;
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Start first section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column">
          <h2 class="title is-3" style="text-align: center;">Limitations in caption-based VLMs</h2>
          <div class="content">
            <p>
              We assess the object localisation capabilities of caption-based VLMs by analysing their textual responses given various prompts. We examine models such as GPT-4V, BLIP-2, Flamingo (using OpenFlamingo), and Fromage. 
              For that, we use prompts aimed at generating a bounding box or text localisation response from these VLMs. Extended analysis can be found in the supplmental.
            </p>
            <!-- Removed is-centered and has-text-centered classes for the image container -->
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/study.jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 20px"/>
              <p style="width: 100%; max-width: 1000px; margin: auto;">
                <i> Examples from our analysis on localisation abilities of existing caption-based VLMs.</i>
              </p>
            </div>
            <p> 
              We find that among the evaluated VLMs, only GPT-4V successfully returns bounding boxes that roughly localise the intended object. Other<strong> VLMs are unable to provide any location information</strong> even in text form and instead are <strong>"chatty"</strong> (FROMAGe, OpenFlamingo) or return the input or provide no output (BLIP-2). 
              One recent stream of research (VisionLLM, UniTab, CogVLM, OFA, MiniGPTv2, mPLUG-Owl, GLIPv2, UnifiedIO, UniTab, Shikra) focuses on developing unified expert VLMs capable of performing a variety of tasks, including localisation, with a universal architecture. Although these models show impressive results across different tasks, their 
              <strong>success largely depends on the availability of extensive task-specific, supervised data</strong>. Furthermore, they typically require a <strong>large amount of compute</strong> for training. The setting we tackle in this paper is different. Our goal is to efficiently enable the localisation capabilities of VLMs while keeping their parameters untouched and without the need for localisation supervised datasets.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  
  <!-- End first section -->

  <!-- Start second section -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Positional Insert (PIN)</h2>
          <div class="content has-text-justified">
            <p>
                We tackle the shortcomings of caption-based Vision-Language Models (VLMs) in their ability to localise objects within images. To this end, we introduce a simple yet effective Positional Insert (PIN), designed to enhance the VLMs' object localisation capabilities without altering their existing parameters or the use of annotated data.
              &nbsp;
            </p>
            <!-- <p> -->
            <!-- Vision-Language Models (VLMs) accept inputs composed of visual data such as images \( I \) alongside a textual input \( T \). The visual component \( I \) is processed by a vision encoder \( \phi_V \) producing a feature vector \( x_v \). Similarly, the textual information \( T \) is tokenized, yielding textual embeddings \( x_{\text{t}} \). The visual features \( x_v\) go through a fusion network \( F \) before being processed with the textual features \( x_t \) to produce a response text \( t_r {=} \text{LLM}(F(x_v), x_t) \) by the Large Language Model. -->
            <!-- </p> -->
            <div class="columns is-centered has-text-centered" style="display: block;">
              <img src="static/images/method.jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px;"/>
              <p style="width: 100%; max-width: 1000px; margin: auto;">
                <i>Overview of our method: PIN is a spatial learnable prompt \( \pi \) which is added to the vision enconding in the forward pass of the VLM.
                    We train PIN with the self-generated supervision signal from our synthetic training data generation.
              </p>
              &nbsp;
            </div>

            <p>
                <strong>PIN is a learnable an input-agnostic spatial feature vector</strong> and is inserted directly after the vision encoder \( \phi_V \). To instill spatial awareness into our PIN module, we start with fixed positional embeddings employing sinusoidal functions. Each of the sinusoidal vectors is further refined by a
                shallow feed-forward neural network \( \psi \), resulting in our PIN \(\pi {=} \psi(S) \). This learned embedding is then added to the output from the vision encoder \( x_v \), resulting in the enriched visual feature representation
                \( x_v^\star = x_v + \pi. \) From there the standard forward pass of the VLM is followed. PIN's parameters are optimized via simple next token prediction by using the bounding box location of a inquired object contained in the text prompt.
            </p>
            <p>
                We do <strong>not rely on manually labeled data</strong> to unlock the positional information in the VLM. Instead, we generate our own synthetic data following XPaste and by utilizing Stable Diffusion to synthesize objects from the LVIS category list. 
                Note, since the vision encoder's weights remain unchanged, it is unlikely to overfit to any pasting artifacts. The composition function \( C \) overlays objects on randomly picked locations while considering the various constraints. 
                This process creates a <strong>self-generated supervision signal</strong> that is subsequently exploited in the training of PIN. 

            </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/train.jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px;"/>
                <p style="width: 100%; max-width: 600px; margin: auto;">
                  <i>Sample images from our synthetic training data.</i>
                </p>
                &nbsp;
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End second section -->



  <!-- Start first section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <p>
                PIN is applied on the Flamingo (using OpenFlamingo) and BLIP-2 VLMs. For COCO, PVOC, and LVIS, localisation is based on ground truth object names. <strong>All results are zero-shot</strong> as overlapping categories with the synthetic training data are removed. We also evaluate PIN for visual grounding on RefCOCO.
            </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/ade (1).jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto; padding-bottom: 30px;">
                    <i>
                    Localisation on a wide range of image types ranging from paintings, and comics to unique scenarios. Despite the varying image content, the enhanced OpenFlamingo VLM with PIN shows strong localisation abilities.
                  </i>
                </p>
              </div>
            
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/coco_pvoc (1).jpg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto; padding-bottom: 30px;">
                  <i>
                    Object localisation results on PVOC and COCO. The PIN module unlocks spatial localisation in the OpenFlamingo VLM.     
                </i>
                </p>
              </div>
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/BLIP.jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto; padding-bottom: 30px;">
                    <i>
                    Object localisation results with BLIP-2 on 224\( \times \)224, BLIP-2 (224), image resolution and 364\( \times \)364, BLIP-2 (364), on PVOC. The PIN trained with the higher image resolution BLIP-2 version is able to predict more accurate bounding boxes.
                  </i>
                </p>
              </div>
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/tab.jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto; padding-bottom: 30px;">
                    Comparison on object localisation on a subset of PVOC, COCO and LVIS with up to 3 objects per image. PIN improves on the OpenFlamingo in-context and PEFT baselines for both the
                OpenFlamingo and BLIP-2 VLM.
                </p>
              </div>

              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/refcoco (1).jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto; padding-bottom: 30px;">
                    <i>
                    Zero-shot visual grounding results on RefCOCO of PIN with the OpenFlamingo VLM. The adapted VLM struggles with more complex scenarios(B and C), yet, it effectively handles simpler cases (F, G, H, J).
                  </i>
                </p>
              </div>
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/refcoco.jpeg" alt="PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs" style="width: 100%; max-width: 400px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto; padding-bottom: 30px;">
                    Evaluation on RefCOCO Test-A. PIN shows decent grounding abilities without using any annotated training data, outperforming the in-context learning Flamingo baseline. Extending our synthetic dataset with positional referrals improves performance.
                </p>
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End first section -->

  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            
            <div class="publication-video">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/QGKc1R5cLjg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End youtube video -->






  <!-- Paper poster -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe  src="static/pdfs/UAI_Poster.pdf" width="100%" height="550">
            </iframe>
          
        </div>
      </div>
    </section> -->
  <!--End paper poster -->

  <section class="section" id="BibTeX" style="background-color: #fafafa;">
    <div class="container is-max-desktop content">
      <div class="columns is-centered">
        <div class="column has-text-centered">
        <h2 class="title">Want to learn more about PIN?</h2>
        <p>Check out our paper and code!</p>
        <div class="publication-links">
          <!-- Arxiv PDF link -->
          <span class="link-block">
            <a href="https://arxiv.org/abs/2402.08657" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-file-pdf"></i>
            </span>
            <span>Paper</span>
            </a>
          </span>

          <!-- Supplementary PDF link -->
          <!-- <span class="link-block">
            <a href="static/pdfs/supplementary_material.pdf" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-file-pdf"></i>
            </span>
            <span>Supplementary</span>
            </a>
          </span> -->

          <!-- Github link -->
          <!-- <span class="link-block">
            <a href="https://colab.research.google.com/github/phlippe/BISCUIT/blob/main/demo.ipynb" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <svg style="color: white" role="img" height="100px" width="100px" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0646 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9297zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9316l2.3911 2.3911a3.6434 3.6434 0 0 1 5.0227.1271l1.7341-2.9737-.0997-.0802A7.033 7.033 0 0 0 7.07 4.9855zm15.0093 2.1721l-2.3892 2.3911a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.4067 2.4068a7.0362 7.0362 0 0 0 9.9456-9.9476zM1.932 7.1674a7.033 7.033 0 0 0-.002 9.6816l2.397-2.397a3.6434 3.6434 0 0 1-.004-4.8916zm7.664 7.4235c-1.38 1.3816-3.5863 1.411-5.0168.1134l-2.397 2.395c2.4693 2.3328 6.263 2.5753 9.0072.5455l.1368-.1115z" fill="white"></path></svg>
            </span>
            <span>Demo</span>
            </a>
          </span> -->

          <!-- Github link -->
          <span class="link-block">
            <a href="https://github.com/QUVA-Lab/PIN/" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>
            <span>Code</span>
            </a>
          </span>

          <!-- Dataset link
          <span class="link-block">
            <a href="https://zenodo.org/record/8027138" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-zenodo"></i>
            </span>
            <span>Datasets</span>
            </a>
          </span> -->

          <!-- ArXiv abstract Link -->
          <!-- <span class="link-block">
            <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
            </a>
          </span> -->
        </div>
      </div>
    </div>
    </div>
  </section>


  <!--BibTex citation -->
  <section class="section" id="BibTeX" style="background-color: #fafafa;">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre style="background-color: #f5f5f5;"><code>
      @misc{dorkenwald2024pin,
            title={PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs}, 
            author={Michael Dorkenwald and Nimrod Barazani and Cees G. M. Snoek and Yuki M. Asano},
            year={2024},
            eprint={2402.08657},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
      }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


    <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>
  
<!-- Default Statcounter code for BISCUIT https://phlippe.github.io/BISCUIT/ -->
<script type="text/javascript">
var sc_project=12891099; 
var sc_invisible=1; 
var sc_security="aad1b357"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12891099/0/aad1b357/1/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>