<!DOCTYPE html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WLNGSQPW7E"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WLNGSQPW7E');
</script>

<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="PIN Project Page">
  <meta property="og:title" content="PIN: Positional Insert unlocks object localisation abilities in VLMs"/>
  <meta property="og:description" content="Project page of 'PIN: Positional Insert unlocks object localisation abilities in VLMs'"/>
  <meta property="og:url" content="https://quva-lab.github.io/PIN/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/first.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="600"/>


  <meta name="twitter:title" content="PIN: Positional Insert unlocks object localisation abilities in VLMs">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/first.png">
  <meta name="twitter:card" content="Positional Insert unlocks object localisation abilities in VLMs.">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="VLMs, object localisation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PIN: Positional Insert unlocks object localisation abilities in VLMs</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body" style="background-color: #fff6e6;">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
                <img src="static/images/pin2.png" alt="PIN Image" style="width:40px;height:40px;"> PIN: Positional Insert unlocks object localisation abilities in VLMs</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mdorkenwald.com/" target="_blank">Michael Dorkenwald</a>,
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/nimrod-barazani/" target="_blank">Nimrod Barazani</a>,
              </span>
              <span class="author-block">
                <a href="https://www.ceessnoek.info/" target="_blank">Cees Snoek*</a>,
              </span>
              <span class="author-block">
                <a href="https://yukimasano.github.io/" target="_blank">Yuki M. Asano*</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">QUVA Lab, University of Amsterdam<br>
                <!-- <span style="font-size: smaller;">*equal last author</span> -->
            </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <!-- <span class="link-block">
                  <a href="https://colab.research.google.com/github/phlippe/BISCUIT/blob/main/demo.ipynb" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg style="color: white" role="img" height="100px" width="100px" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0646 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9297zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9316l2.3911 2.3911a3.6434 3.6434 0 0 1 5.0227.1271l1.7341-2.9737-.0997-.0802A7.033 7.033 0 0 0 7.07 4.9855zm15.0093 2.1721l-2.3892 2.3911a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.4067 2.4068a7.0362 7.0362 0 0 0 9.9456-9.9476zM1.932 7.1674a7.033 7.033 0 0 0-.002 9.6816l2.397-2.397a3.6434 3.6434 0 0 1-.004-4.8916zm7.664 7.4235c-1.38 1.3816-3.5863 1.411-5.0168.1134l-2.397 2.395c2.4693 2.3328 6.263 2.5753 9.0072.5455l.1368-.1115z" fill="white"></path></svg>
                  </span>
                  <span>Demo</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/QUVA-Lab/PIN/" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                </span>

                <!-- Dataset link
                <span class="link-block">
                  <a href="https://zenodo.org/record/8027138" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-zenodo"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <!-- <section class="hero teaser" style="background-color: #fff6e6;">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%" style="background-color: white; padding: 30px;">
          <source src="static/videos/banner_video.mov"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <b>BISCUIT</b> learns causal variables from interactivate environments with low-level action information.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">
        
      <div class="columns is-centered has-text-centered">
        
        <div class="column is-four-fifths">

          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
                Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not utilizing any supervised detection data. 
                To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons. 
            </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/first_page.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 500px; padding-top: 10px;"/>
                <p style="width: 100%; max-width: 600px; margin: auto;">
                We learn a single Positional Insert (PIN) for unlocking zero-shot object localisation abilities in a frozen Vision Language Model (VLM) without adding any additional heads or requiring supervised datasets.</i>
                </p>
                &nbsp;
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Start first section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Limitations in caption-based VLMs</h2>
          <div class="content has-text-justified">
            <p>
                We assess the object localisation capabilities of caption-based VLMs by analysing their textual responses given various prompts. We examine models such as GPT-4V, BLIP-2, Flamingo (using OpenFlamingo), and Fromage. For that, we use prompts aimed at generating a bounding box response from these VLMs.
                We find that among the evaluated VLMs, only GPT-4V successfully returns bounding boxes that roughly localise the intended object. Other VLMs are unable to provide any location information even in text form and instead are ``chatty'' (FROMAGe, OpenFlamingo) or return the input or provide no output (BLIP-2). 
                </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/study.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <!-- <p style="width: 100%; max-width: 1000px; margin: auto;"> -->
                  <!-- <i><b>BISCUIT</b> identifies causal variables from images by learning to encode low-level action information to binary interaction variables.</i> -->
                <!-- </p> -->
              </div>
            
            <p>
            We also quantitatively evaluate the in-context learning abilities for localisation of the OpenFlamingo model and broaden our study by examining a wider variety of prompts, specifically including those that do not require generating a bounding box, and by analyzing a larger number of samples. 
            Yet, the conclusion remains the same as with the exemplary results here that caption-based VLMs are unable to localise objects in a given image via textual responses. 
            </p>
            <p> 
            One recent stream of research (VisionLLM, UniTab, CogVLM, OFA, MiniGPTv2, mPLUG-Owl, GLIPv2, UnifiedIO, UniTab, Shikra) focuses on developing unified expert Vision Language Models (VLMs) capable of performing a variety of tasks, including localisation, with a universal architecture. 
            Although these models show impressive results across different tasks, their success largely depends on the availability of extensive task-specific, supervised data. Furthermore, many require a large amount of compute for training.
            The setting we tackle in this paper is different. Our goal is to efficiently enable the localisation capabilities of VLMs while keeping their parameters untouched and without the need for localisation supervised datasets.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End first section -->

  <!-- Start second section -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Positional Insert (PIN)</h2>
          <div class="content has-text-justified">
            <p>
                We tackle the shortcomings of caption-based Vision-Language Models (VLMs) in their ability to localise objects within images. To this end, we introduce a simple yet effective Positional Insert (PIN), designed to enhance the VLMs' object localisation capabilities without altering their existing parameters or the use of annotated data.
              &nbsp;
            </p>
            <p>
            Vision-Language Models (VLMs) accept inputs composed of visual data such as images \( I \) alongside a textual input \( T \). The visual component \( I \) is processed by a vision encoder \( \phi_V \) producing a feature vector \( x_v \). Similarly, the textual information \( T \) is tokenized, yielding textual embeddings \( x_{\text{t}} \). The visual features \( x_v\) go through a fusion network \( F \) before being processed with the textual features \( x_t \) to produce a response text \( t_r {=} \text{LLM}(F(x_v), x_t) \) by the Large Language Model.
            </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
              <img src="static/images/method.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px;"/>
              <p style="width: 100%; max-width: 600px; margin: auto;">
                <i><b>BISCUIT</b> identifies causal variables from images by learning to encode low-level action information to binary interaction variables.</i>
              </p>
              &nbsp;
            </div>

            <p>
                PIN is a learnable an input-agnostic spatial feature vector and is inserted directly after the vision encoder \( \phi_V \). To instill spatial awareness into our PIN module, we start with fixed positional embeddings of dimension \( d \) employing sinusoidal functions. Each of the spatial sinusoidal vectors is further refined by a
                learnable, shallow feed-forward neural network \( \psi \) parametrized by \( \theta \), resulting in our PIN \(\pi {=} \psi(S) \) with the output dimension matching the ones from the vision encoder \( x_v \). This learned embedding is then added to the output from the vision encoder \( x_v \), resulting in the enriched visual feature representation
                \( x_v^\star = x_v + \pi. \) From there the standard forward pass of the VLM is followed.
            </p>
            <p>
                We do not rely on manually labeled data to unlock the positional information in the VLM. Instead, we generate our own synthetic data following XPaste and by utilizing Stable Diffusion to synthesize objects from the LVIS category list. 
                Note, since the vision encoder's weights remain unchanged, it is unlikely to overfit to any pasting artifacts. The composition function \( C \) overlays objects on randomly picked locations while considering the various constraints. This process creates a self-generated supervision signal that is subsequently exploited in the training of PIN. 

            </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/train.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px;"/>
                <p style="width: 100%; max-width: 600px; margin: auto;">
                  <i><b>BISCUIT</b> identifies causal variables from images by learning to encode low-level action information to binary interaction variables.</i>
                </p>
                &nbsp;
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End second section -->



  <!-- Start first section -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <p>
                We analyse blabla
            </p>
            <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/ade (1).jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto;">
                  <i><b>BISCUIT</b> identifies causal variables from images by learning to encode low-level action information to binary interaction variables.</i>
                </p>
              </div>
            
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/coco_pvoc (1).jpg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto;">
                  <i><b>BISCUIT</b> identifies causal variables from images by learning to encode low-level action information to binary interaction variables.</i>
                </p>
              </div>
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/BLIP.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto;">
                  <i><b>BISCUIT</b> identifies causal variables from images by learning to encode low-level action information to binary interaction variables.</i>
                </p>
              </div>
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/tab.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto;">
                Comparison on object localisation on a subset of PVOC, COCO and LVIS with up to 3 objects per image, yielding 3,582, 2,062 and 6,016 test images respectively. PIN improves on the OpenFlamingo in-context and PEFT baselines for both the
                OpenFlamingo and BLIP-2 VLM.
                </p>
              </div>

              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/refcoco.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 1000px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto;">
                  <i><b>BISCUIT</b> identifies causal variables from images by learning to encode low-level action information to binary interaction variables.</i>
                </p>
              </div>
              <div class="columns is-centered has-text-centered" style="display: block;">
                <img src="static/images/refcoco_tab.jpeg" alt="PIN: Positional Insert unlocks object localisation abilities in VLMs" style="width: 100%; max-width: 500px; padding-top: 10px"/>
                <p style="width: 100%; max-width: 1000px; margin: auto;">
                Evaluation on RefCOCO Test-A. PIN shows decent grounding abilities without using any annotated training data, outperforming the in-context learning Flamingo baseline. Extending our synthetic dataset with positional referrals improves performance considerably.
                </p>
              </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End first section -->

  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            
            <div class="publication-video">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/QGKc1R5cLjg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End youtube video -->






  <!-- Paper poster -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe  src="static/pdfs/UAI_Poster.pdf" width="100%" height="550">
            </iframe>
          
        </div>
      </div>
    </section> -->
  <!--End paper poster -->

  <section class="section" id="BibTeX" style="background-color: #fafafa;">
    <div class="container is-max-desktop content">
      <div class="columns is-centered">
        <div class="column has-text-centered">
        <h2 class="title">Want to learn more about PIN?</h2>
        <p>Check out our paper and code!</p>
        <div class="publication-links">
          <!-- Arxiv PDF link -->
          <span class="link-block">
            <a href="https://arxiv.org/abs/2306.09643" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-file-pdf"></i>
            </span>
            <span>Paper</span>
            </a>
          </span>

          <!-- Supplementary PDF link -->
          <!-- <span class="link-block">
            <a href="static/pdfs/supplementary_material.pdf" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fas fa-file-pdf"></i>
            </span>
            <span>Supplementary</span>
            </a>
          </span> -->

          <!-- Github link -->
          <!-- <span class="link-block">
            <a href="https://colab.research.google.com/github/phlippe/BISCUIT/blob/main/demo.ipynb" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <svg style="color: white" role="img" height="100px" width="100px" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0646 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9297zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9316l2.3911 2.3911a3.6434 3.6434 0 0 1 5.0227.1271l1.7341-2.9737-.0997-.0802A7.033 7.033 0 0 0 7.07 4.9855zm15.0093 2.1721l-2.3892 2.3911a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.4067 2.4068a7.0362 7.0362 0 0 0 9.9456-9.9476zM1.932 7.1674a7.033 7.033 0 0 0-.002 9.6816l2.397-2.397a3.6434 3.6434 0 0 1-.004-4.8916zm7.664 7.4235c-1.38 1.3816-3.5863 1.411-5.0168.1134l-2.397 2.395c2.4693 2.3328 6.263 2.5753 9.0072.5455l.1368-.1115z" fill="white"></path></svg>
            </span>
            <span>Demo</span>
            </a>
          </span> -->

          <!-- Github link -->
          <span class="link-block">
            <a href="https://github.com/phlippe/BISCUIT" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>
            <span>Code</span>
            </a>
          </span>

          <!-- Dataset link
          <span class="link-block">
            <a href="https://zenodo.org/record/8027138" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-zenodo"></i>
            </span>
            <span>Datasets</span>
            </a>
          </span> -->

          <!-- ArXiv abstract Link -->
          <!-- <span class="link-block">
            <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
            class="external-link button is-normal is-rounded is-dark">
            <span class="icon">
              <i class="ai ai-arxiv"></i>
            </span>
            <span>arXiv</span>
            </a>
          </span> -->
        </div>
      </div>
    </div>
    </div>
  </section>


  <!--BibTex citation -->
  <section class="section" id="BibTeX" style="background-color: #fafafa;">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre style="background-color: #f5f5f5;"><code>

      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


    <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>
  
<!-- Default Statcounter code for BISCUIT https://phlippe.github.io/BISCUIT/ -->
<script type="text/javascript">
var sc_project=12891099; 
var sc_invisible=1; 
var sc_security="aad1b357"; 
</script>
<script type="text/javascript"
src="https://www.statcounter.com/counter/counter.js" async></script>
<noscript><div class="statcounter"><a title="Web Analytics"
href="https://statcounter.com/" target="_blank"><img class="statcounter"
src="https://c.statcounter.com/12891099/0/aad1b357/1/" alt="Web Analytics"
referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>
<!-- End of Statcounter Code -->

</body>
</html>